\chapter{基于纠删码对象存储的分布式文件存储}
\section{引言}
对于基于纠删码编码的分布式对象存储系统来说，由于纠删码编码和解码的计算开销很大，现有的分布式文件系统在对象存储上直接进行对象的修改操作，不仅会增加系统设计的复杂性，降低系统的通用性，而且由于在对象修改时需进行解码和编码，会导致系统的效率低下。因此，基于纠删码实现的对象存储一般假设对象是只读的，故并不能将存储系统中的对象直接封装为文件供分布式文件系统使用。

我们设计的基于纠删码的分布式文件系统以只读的对象存储系统为基础，将对文件的修改操作映射为对原始对象的修改事件，并将修改操作同样作为对象新增存储于对象存储引擎中，从而在逻辑上对原始对象进行了修改，简化了对象存储的设计，并减少了文件修改时的纠删码解码和重新编码的开销。此修改算法实现了文件的修改和附加等常见操作，从而可以基于此动态纠删码对象存储系统上实现典型的块存储、文件存储和数据库存储系统，以满足不同场景下的高性能高可用的分布式存储需求。
\section{基于纠删码的分布式存储系统}
\subsection{基于纠删码编码的对象存储}
在基于纠删码的分布式存储系统中，为了减少由于纠删码的编码和解码带来的计算开销，可以基于只读的分布式对象存储对数据进行存储。

对于待存储的对象，我们先使用 On-demand ARECS 算法选取校验码块数 $m$ 满足下式的纠删码编码方案 $(n+m,n)$：
$$
m(n)=\left\lceil\operatorname*{argmin}_{m}f(n,m)\right\rceil,\ \operatorname*{where}\ f(n,m) \geq 0
$$
再选取其中总读写时间 $t_{f}(n+m)$ 取最小值时的磁盘组：
$$
S_{d}'(n+m)=\operatorname*{argmin}_{S_{d}'(n+m)}\left(t_{calc}+\max_{d_{i}{\in}S_{d}'(n+m)}t_{d_i}+(r-1)\left(\max\left(t_{calc},\max_{d_{i}{\in}S_{d}'(n+m)}t_{d_i}\right)\right)\right)
$$

我们将待存储的数据对象分为 $n$ 份原始数据块，并使用里德-所罗门码纠删码编码生成 $m$ 份冗余备份块。对于这 $n+m$ 块数据，我们将其分别存放于选取的磁盘组 $S_{d}'(n+m)$ 中的 $n+m$ 块磁盘中。这样，我们便可以实现基于纠删码的快速的对象存储。

在数据的存储过程中，由于存储的数据块是只读的，因此每块数据块的长度是固定不变的。根据上一章中的研究，由于较大的对象在纠删码编码时存在由于编码矩阵过大，导致计算效率低下的问题，故在大对象存储时一般将其切分为较小的大小确定且均等的数据块。因此，我们在存储数据块时，可以不通过本地文件系统，直接将存储设备的线性地址空间分为若干个大小确定且均等的数据簇，每个数据簇存储一个数据块。这样，一个数据块就可以直接使用存储设备编号和数据簇编号来进行索引，避免了本地文件系统带来的性能损失，从而提高系统的并行性。
\subsection{基于对象存储的分布式文件系统中间件}
由于目前的分布式数据库以及云计算多需要一个支持修改操作的存储后端，上述只读的分布式对象存储不能满足这些存储领域的需求。因此，为了分布式存储系统的通用性，我们在基于纠删码实现的对象存储的基础上，设计了一个支持文件修改（Modify）和附加（Append）操作的存储引擎，来满足目前大型数据中心中多样化的存储需求。为了实现方便，此文件系统基于 Linux 的虚拟文件系统（Virtual File System，即 VFS）进行设计和实现。

VFS 子系统是 Linux 内核提供的一个灵活的文件系统框架\cite{wetzel2017virtual}。在 Linux 操作系统上，所有和文件系统相关的系统调用，均被提交至 VFS 子系统，由 VFS 子系统转发相应的内核模块。实际的文件系统模块可以是 ext4 或 XFS 等基于磁盘存储的文件系统，也可以是 proc 或 sysfs 等提供系统服务的虚拟的文件系统。我们通过实现一个新的虚拟文件系统模块作为中间件，截获 Linux 操作系统中应用程序的文件系统调用，再把其转换为基于纠删码的分布式对象存储系统的读写请求，使得各种 Linux 应用程序可以和使用原生文件系统一样，不做修改地支持分布式文件存储系统。

\begin{figure}[!htb]
\centering
\resizebox{.8\textwidth}{!}{\input{figures/figure3}}
\caption{基于 FUSE 框架实现的文件系统架构图}
\label{p3}
\end{figure}

在基于 Linux 虚拟文件系统的分布式文件系统接口实现中，我们采用了用户空间文件系统（Filesystem in Userspace，即 FUSE）框架，如图 \ref{p3} 所示。我们通过 FUSE 在用户态创建了一个分布式文件系统的服务进程，实现文件系统的标准 POSIX 接口，并将其转换为基于纠删码的分布式对象存储系统的读写请求调用。此文件系统主要分为两大部分，文件系统的元数据和文件内容数据。类似于分布式文件系统 Ceph 的方案，我们将元数据存储和文件内容数据存储分离，元数据采取单独的存储池和数据结构进行存储，文件内容数据作为对象存储于分布式对象存储系统中。

在分布式文件系统的元数据存储部分，我们存储的分布式文件系统元数据包含文件的修改时间和权限等属性信息，以及文件在分布式对象存储中的对象索引列表。由于分布式文件系统的元数据在文件的 I/O 操作中需要频繁读写，因此在分布式文件系统进行挂载时，服务进程通过对文件系统的元数据进行本地缓存，从而优化文件的读写速度，保障文件系统元数据的快速读写。

在分布式文件系统的文件内容数据存储部分，我们使用修改事件存储文件的内容数据。在文件每次进行创建和修改时，将其新增或修改的内容均单独作为一个对象，给予编号存储于分布式对象存储中。在文件进行读取时，通过寻找读取位置最近的修改记录，确定待读取数据所位于的对象。通过这种方式，我们避免了在文件每次进行修改的重新编码和解码操作，从而提高系统效率。
\section{基于修改事件的分布式文件修改算法}
\subsection{基于修改事件的文件数据存储结构}
对于基于修改事件的分布式文件系统，在文件创建、修改或附加等事件发生时，我们将每个修改事件均描述为一个修改事件。下面我们详细介绍修改事件的存储方式。
\subsubsection{随机对象唯一编号的生成方案}
首先，我们需要为每个修改事件均分配一个随机的对象唯一编号，在此记为 UUID。由于在文件读取时需要获知文件修改的先后顺序，从而确定最近的修改记录，因此 UUID 需要包含修改事件的高精度时间。为了保证系统的通用性，我们不妨使用根据 RFC 4122 标准生成的 UUID，其结构如图 \ref{p16} 所示。在 UUID 的生成中，我们采用的版本号为 1，变体号为 1，时间戳是一个 60 比特的数据，记录从协调世界时（Coordinated Universal Time，即 UTC）1582 年 10 月 15 日 00 时 00 分 00 秒（即首次采用格里高利历的日期）至今所经过的 100 纳秒数，时钟序列是一个 14 比特的数据，在系统时间戳倒退时进行相应的增加，节点则是一个 48 比特的数据，存储当前节点编号，通常使用网卡的物理地址（Media Access Control Address，即 MAC 地址）。

\begin{figure}[!htb]
\centering
\resizebox{.8\textwidth}{!}{\input{figures/figure16}}
\caption{对象唯一编号结构示意图}
\label{p16}
\end{figure}
\subsubsection{文件修改对象示意}
在按照 RFC 4122 标准生成 UUID 作为对象的唯一编号之后，我们将修改的后的文件内容作为单独的对象，以生成的唯一编号 UUID 作为索引，存储于基于纠删码的对象存储系统中。对于文件修改和文件附加操作，存储的对象相对于原始文件示意如图 \ref{p4} 所示。

\begin{figure}[!htb]
\centering
\resizebox{.8\textwidth}{!}{\input{figures/figure4}}
\caption{修改事件对象示意}
\label{p4}
\end{figure}
\subsubsection{文件元数据的存储结构}
对于存储的文件修改对象，我们需要在分布式文件系统的元数据中对其进行索引。我们首先需要在元数据中存储此次修改事件内容的编号 UUID，从而记录文件变更后的内容。此外，我们还需要存储此变更所参照的原始文件，即编号 UUID$_{prev}$，从而确定文件修改的参照点。我们还需要记录此修改或者附加的起始位置 start $=$ offset 和终止位置 end $=$ offset $+$ length，其中 length 为编号 UUID 所对应的对象大小。我们采用如图 \ref{p15} 的结构存储这些数据，从而得到一个完整的文件修改记录，并计算文件的最新内容。

\begin{figure}[!htb]
\centering
\resizebox{.8\textwidth}{!}{\input{figures/figure15}}
\caption{分布式文件系统的元数据结构}
\label{p15}
\end{figure}

在读取文件时，我们可以遍历这个元数据结构，选择所有 UUID 编号中时间戳和时钟序列最新的对象，来确定当前最新版本的文件。若文件待读取的位置不在上述 UUID 对应记录的 start 和 end 之间，则递归地按上述步骤读取 UUID$_{prev}$ 对应的记录，直至找到符合条件的记录，并返回符合条件的记录的 UUID 对应的对象中的相应位置。此算法流程图如 \ref{p21} 所示。

\begin{figure}[!htb]
\centering
\resizebox{.8\textwidth}{!}{\input{figures/figure21}}
\caption{文件读取流程图}
\label{p21}
\end{figure}
\subsubsection{文件的版本历史和并发写入}
通过上述文件修改事件结构，我们可以维护一个文件的历史版本记录，在文件打开操作时，我们确定一个最新的文件版本，在此后对该文件的操作中，均针对此版本的文件进行修改，因此可以保证并发的写入请求不发生冲突。如果存在多个进程同时操作同一个文件，便会在文件系统中同时存储该文件的多个版本，文件系统将选择最新修改的文件版本。
\subsection{文件修改事件元数据的无锁更新算法}
在元数据的修改过程中，为了保证数组的一致性，若有多个线程同时对数组进行更新，则可能导致数据的不一致。为了防止对共享数据的非法访问，一般可以采用自旋锁等结构对并发访问进行限制。但采用锁机制会产生额外的性能开销。由于我们的数据结构和访问模式较为固定，在此处我们采用基于数组的无锁队列进行元数据的更新。
\subsubsection{无锁队列的结构}
在基于数组的无锁队列中，我们采用数组下标 $read\_index$ 和 $write\_index$ 来分别表示当前数组的读下标和写下标。在数据初始化时，这两个下标均初始化为 0，即数据的第一个元素位置。在数据插入时，我们使用原子操作，将 $write\_index$ 加 1，并返回原来的 $write\_index$ 值作为写入位置 $write\_current$。之后我们便可以向数组的 $write\_current$ 位置写入数据。在数据写入完成后，我们使用原子操作，比较若 $read\_index$ 与 $write\_current$ 相同，则将 $read\_index$ 改为 $write\_current+1$，否则不进行任何操作。若执行此步骤后 $read\_index \leq write\_current$，则重新执行上面一步，直至 $read\_index>write\_current$。整个数据插入过程如图 \ref{p19} 所示。

\begin{figure}[!htb]
\centering
\resizebox{.8\textwidth}{!}{\input{figures/figure19}}
\caption{无锁队列的数据插入示意}
\label{p19}
\end{figure}
\subsubsection{无锁队列的并发插入}
通过采用此无锁队列，我们在更新对象元数据时，可以直接将修改对象的编号等信息插入到元数据末端。在多个进程进行并发写入时，此队列可以做到无锁的并发写入，如图 \ref{p20} 所示，在一个进程完成数据写入之前，又有另一个进程申请写入数据。对于这种情况，基于数组的无锁队列中的 $write\_current$ 向前继续移动一个元素，然后两个进程可以分别在对应位置写入，不会造成冲突。在数据写入完成之后，在对 $read\_index$ 递增时，第二个写入进程必须等待第一个写入进程完成之后才能进行递增，因为我们必须保证数据完全被写入后才能被读取。基于此无锁队列的元数据存储在读取文件时，即进行元数据遍历等读取操作时，可以读取的数据便是无锁队列中已经完成写入的数据，也就是从上述队列头部开始，到 $read\_index$ 前的位置为止之间的数据。

\begin{figure}[!htb]
\centering
\resizebox{.8\textwidth}{!}{\input{figures/figure20}}
\caption{无锁队列中多进程同时写入示意}
\label{p20}
\end{figure}
\subsection{文件修改事件对象的合并算法}
基于文件修改对象的存储方式虽然能加速文件的写入速度和并行度，但当文件修改次数较多时，文件的读取速度将无法保证。在大部分情况下，我们不需要文件的修改历史，仅需要最新版本的文件内容。因此我们计划在系统空闲时，通过对修改历史较多的对象进行重新构建，防止在数据读取时需要遍历过多版本的数据。
\subsubsection{对象合并的方式}
通过扫描文件系统中文件的未合并对象数，我们将未合并对象数大于一定阈值的文件进行整文件读取，然后将读取的对象作为一个全新的对象进行存储。在更换对象时，我们通过比对最新的记录与合并前的记录是否相同，确保对象在被读取到被替换这段时间未被修改，从而做到文件的无锁更新，并避免在更新过程中修改文件的操作导致对象不一致现象的产生。

我们可以建立一个可删除对象列表，在每次合并算法执行过后，将早于合并算法执行时间的对象编号存储于此列表中，然后选取一个阈值，若磁盘占用率大于规定阈值，则自动清理可删除对象列表中的对象。在用户手动执行删除操作时，我们亦可以清理此可删除对象列表中的对象，从而避免空间浪费。
\subsubsection{对象合并的执行时机}
由于文件修改事件的合并和回收算法均需要耗费较长的时间，我们希望此算法仅在系统空闲时才会执行。我们计划通过检测后端对象存储的读写时间 $t_{f}$ 和实际的对象存储带宽 $b_{w}$，来获知后端存储的使用情况。在检测到后端存储空闲时，再进行数据写入，即：

\begin{itemize}
\item 根据对象存储的读写时间 $t_{f}$ 设定阈值 $t_{f}'$，在 $t_{f}<t_{f}'$ 时发送数据整合请求，直至系统的 $t_{f} \geq t_{f}'$ 为止
\item 根据系统的对象存储带宽 $b_{w}$ 和最大带宽 $\max(b_{w})$ 设定阈值 $b_{w}'$，$0<b_{w}'<\max(b_{w})$，在 $b_{w}<b_{w}'$ 时发送数据整合请求，直至系统的 $b_{w} \geq b_{w}'$ 为止
\end{itemize}

这两种方案均设定了一个硬阈值来代表系统是否空闲，但在实际应用中，系统的负载情况是动态变化的，因此此方案过于依赖阈值的设置，容易造成存储性能浪费或数据整合请求堆积。因此，我们可以通过系统状态动态地估算阈值，以此来平衡系统的读性能和写性能。我们定义对象 $f$ 的写入列表的集合为 $S_{w}(f)$，数据整合的带宽需求为：
$$
b(f)=\sum_{w{\in}S_{w}(f)}length(data)_{w}
$$

全部对象的带宽需求总和为：
$$
b_{f}=\sum_{f{\in}\{f|\|S_{w}(f)\|{\ne}1\}}b(f)
$$

如果考虑写入事件数为 $n_{w}=\|S_{w}(f)\|$，显然数据整合请求的优先级为 $n_{w}$ 越高，对读性能的影响越大，因此数据整合的优先级越高，定义优先级可以表示为函数 $priority(n_{w})$，因此定义：
$$
b_{f}'=\sum_{f{\in}\{f|\|S_{w}(f)\|{\ne}1\}}priority(\|S_{w}(f)\|)*b(f)
$$

因此阈值应为函数 $t_{f}'=t_{f}'(b_{f}')$ 和 $b_{w}'=b_{w}'(b_{f}')$。我们需要平衡不同 $priority(n_{w})$ 和 $t_{f}'(b_{f}')$ 以及 $b_{w}'(b_{f}')$ 的性能表现，从而动态平衡系统的读性能和写性能。
\section{实验与分析}
\subsection{实验环境}
在本章的实验中，我们继续采用如表 \ref{t2} 和图 \ref{p6} 所示的基于 KVM 虚拟化技术的由 $37$ 个节点组成的分布式文件系统实验环境。在此分布式文件系统实验环境中，我们在上文的基于 On-demand ARECS 算法的 Tahoe-LAFS 分布式文件系统中，实现了基于修改事件的文件存储算法，并针对文件的修改和文件的附加两种操作，分别对算法进行了测试。
\subsection{文件修改实验结果与分析}
在文件修改操作的测试过程中，我们随机生成了一个大小为 50 MB 的文件作为未修改前的原始文件，然后让文件系统每次对该文件中的任意 10 MB 内容进行随机修改，测量并记录得到原始文件修改算法相比于我们的算法的总执行时间随修改数变化如图 \ref{p18} 所示。

\begin{figure}[!htb]
\centering
\resizebox{.8\textwidth}{!}{\input{figures/figure18}}
\caption{算法的执行时间随修改事件数变化}
\label{p18}
\end{figure}

通过图 \ref{p18} 可以看出，使用基于修改事件的文件存储算法，在文件修改操作中由于避免了文件的解码操作，并减少了文件重复部分的编码操作，从而大大降低了文件修改所需要的时间，对于进行 1 到 15 次的文件修改操作来说，将平均存储时间降低到了原来的 $31\%$。
\subsection{文件附加实验结果与分析}
在文件附加操作的测试过程中，我们创建一个空的新文件，然后对该文件进行多次附加操作，文件的每次附加操作在文件末尾追加 10 MB 的随机内容，测量并记录得到原始文件修改算法相比于我们的算法的执行时间随附加事件数变化如图 \ref{p17} 所示。

\begin{figure}[!htb]
\centering
\resizebox{.8\textwidth}{!}{\input{figures/figure17}}
\caption{算法的执行时间随附加事件数变化}
\label{p17}
\end{figure}

通过图 \ref{p17} 可以看出，在文件附加操作中使用基于修改事件的文件存储算法，由于基本避免了原有文件的编码和解码操作，在大文件追加时相比于直接修改对象来说性能有很大提高。在小文件追加时，由于即使对象存储引擎对于原有对象进行了纠删码的解码与重新编码，由于数据量不大，因此性能提高幅度较小。总体来说，在进行 1 到 20 次附加操作的实验中，基于修改对象的文件存储算法将文件存储时间平均降低到了原来的 $41\%$。
\section{小结}
在本章中，我们首先介绍了基于纠删码的对象存储及其物理存储方式，然后介绍了在 Linux 系统中通过 FUSE 技术实现一个基于对象存储的分布式文件系统中间件的方法。针对基于纠删码的对象存储在对象修改时需要解码和重新编码，效率低下的特点，我们采用修改事件表示文件修改操作，并介绍了相应的文件存储和读取算法。针对文件存储时元数据频繁更新的性能瓶颈问题，我们通过引入基于数组的无锁队列，避免元数据写入时锁的开销，提高系统的并发性能。针对多次修改后的文件读取时遍历比较次数过多的问题，我们介绍了在系统空闲时间对修改事件进行无锁合并的策略。我们在基于 On-demand ARECS 算法的 Tahoe-LAFS 分布式文件系统上实现了此修改算法并进行了对比实验，实验结果表明，基于修改事件的分布式文件存储算法相比于直接修改原始对象，在纠删码对象存储引擎上的文件修改和附加操作平均时间降低到了原来的 $30\%\sim40\%$。
